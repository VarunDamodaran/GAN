{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "uecUSvU7FUT_",
        "outputId": "7768eb8f-6ba8-46ad-fd64-797c16fd71f2"
      },
      "outputs": [],
      "source": [
        "!pip3 install tensorflow==2.8.0\n",
        "!pip3 install keras==2.6.0\n",
        "!pip install matplotlib==3.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YI-KS-uaN4Nr"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_aZtVKSARhC"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense, Input, Reshape, Flatten\n",
        "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "# from keras.optimizers_v1 import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-e3rjtoGgBe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YEtdk3xFSGi"
      },
      "outputs": [],
      "source": [
        "colab_directory = \"/content/gan_images2\"\n",
        "os.makedirs(colab_directory, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPhMiJOUFSAu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GbGKEIMrEHIW"
      },
      "outputs": [],
      "source": [
        "img_rows = 28\n",
        "img_columns = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_columns, channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEzdDwTYJvVq"
      },
      "outputs": [],
      "source": [
        "#generator\n",
        "#input is a noise vector and output is a fake (generated ) image\n",
        "def build_generator():\n",
        "  noise_shape = (100,) #100*1 1d noise vector as input\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add( Dense(256, input_shape=noise_shape))\n",
        "  model.add( LeakyReLU(alpha=0.2))\n",
        "  model.add( BatchNormalization(momentum=0.8))\n",
        "  model.add( Dense(512))\n",
        "  model.add( LeakyReLU(alpha=0.2))\n",
        "  model.add( BatchNormalization(momentum=0.8))\n",
        "  model.add( Dense(1024))\n",
        "  model.add( LeakyReLU(alpha=0.2))\n",
        "  model.add( BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "\n",
        " # model.summary()\n",
        "  noise = Input(shape=noise_shape)\n",
        "  img = model(noise) #img stores the output of this model, given the noise vector as the input\n",
        "  return Model(noise, img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjDZ5TzIJx3O"
      },
      "outputs": [],
      "source": [
        "yo = build_generator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7ngPq_RMKi7"
      },
      "outputs": [],
      "source": [
        "# takes input as an image and outputs the prob of it being a valid(real) image (as predicted by it)\n",
        "def build_discriminator():\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # model.summary()\n",
        "\n",
        "  img = Input(shape=img_shape)\n",
        "  validity = model(img)\n",
        "\n",
        "  return Model(img, validity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KazWl6jaMhJy"
      },
      "outputs": [],
      "source": [
        "def train(epochs, batch_size=32, save_interval=50):\n",
        "  norm = 255.0/2\n",
        "  (X_train, _), (_, _) = mnist.load_data()\n",
        "  X_train = (X_train.astype(np.float32) - norm)/norm\n",
        "  X_train = np.expand_dims(X_train, axis = 3) #xtrain has say 50k images each of 28*28, so it is 50k * 28 * 28,\n",
        "  #we use np. so that each element is now of 3 dimensions, so each is 28*28*1\n",
        "  half_batch = int(batch_size / 2)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    ind = np.random.randint(0, X_train.shape[0], half_batch) #random indexes (half_batch jitne) from 0 to len of xtrain - 1 , for explaination say half batch\n",
        "    #is 50 , so 50 random images choose karne ke liye\n",
        "    images = X_train[ind] #these 50 images\n",
        "\n",
        "  #noise vectors generation : each vector ke liye 100 wala , so i need 50 vectors of size 100\n",
        "    noise = np.random.normal(0, 1, (half_batch, 100)) # one row for each image\n",
        "    generated_images = generator.predict(noise) #predict 50 random images for these noise vectors\n",
        "\n",
        "    #now, train discriminator:\n",
        "    # 50 fake imags with target 0, and 50 real images with target 1\n",
        "    d_loss_real = discriminator.train_on_batch(images, np.ones((half_batch, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_images, np.zeros((half_batch, 1)))\n",
        "\n",
        "    d_loss = 0.5*(np.add(d_loss_real, d_loss_fake))\n",
        "\n",
        "    #now train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    valid_output = np.array([1]*batch_size) #here all images so batch_size\n",
        "    g_loss = combined.train_on_batch(noise, valid_output)\n",
        "\n",
        "    print(\"epoch : \", epoch, \" d loss avg : \", d_loss, \"accuracy % :\", 100*d_loss[1], \"g loss: \", g_loss) #loss is returned as a tuple : loss, accuracy\n",
        "\n",
        "    if epoch % save_interval==0:\n",
        "      save_images(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxnFUpRHMwOP"
      },
      "outputs": [],
      "source": [
        "def save_images(epoch):\n",
        "  r , c= 5, 5\n",
        "  noise = np.random.normal(0, 1, (r*c, 100)) #will test our generator after some no of epochs, by generating 25 random images\n",
        "  generated_images = generator.predict(noise) #get the images (generated)\n",
        "  generated_images = 0.5 * generated_images + 0.5 #scale the images (received from tanh so [-1, 1] so *0.5 -> [-0.5, +0.5] +0.5 -> [0, 1])\n",
        "\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt =0\n",
        "  for i in range (r):\n",
        "    for j in range (c):\n",
        "      axs[i, j].imshow(generated_images[cnt, :, :, 0], cmap='gray')\n",
        "      axs[i, j].axis('off')\n",
        "      cnt+=1\n",
        "  fig.savefig(os.path.join(colab_directory, \"mnist_%d.png\" % epoch))\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyMdk0bZDsWi"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(0.0002, 0.5)\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator()\n",
        "generator.compile(loss = 'binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "z = Input(shape=(100,))\n",
        "img = generator(z)\n",
        "discriminator.trainable = False\n",
        "\n",
        "valid = discriminator(img)\n",
        "combined = Model(z, valid)\n",
        "combined.compile(loss = 'binary_crossentropy', optimizer=optimizer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0diO7I6tJrMW",
        "outputId": "a5949463-e42e-425f-dc52-6da1bcdafe7d"
      },
      "outputs": [],
      "source": [
        "train(epochs =10000,batch_size = 32, save_interval =30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK024Ya2KPXw"
      },
      "outputs": [],
      "source": [
        "generator.save('generator.h5')\n",
        "discriminator.save('discriminator.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky3Qn853S0V8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
